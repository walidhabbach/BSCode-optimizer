{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Set up HuggingFace access token as an EV**"
      ],
      "metadata": {
        "id": "fIOCjC0iECVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Hugging Face token from user data\n",
        "from google.colab import userdata\n",
        "import os\n",
        "# In the left side bar, you can find a key logo, click on it and create your Hugging Face access token key variable\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN"
      ],
      "metadata": {
        "id": "WpHbimROeW8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Required Libraries**"
      ],
      "metadata": {
        "id": "DMQ-EtHOoAAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "transformers\n",
        "flask\n",
        "pyngrok\n",
        "accelerate\n",
        "torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc9cIAKtA2t9",
        "outputId": "1fe5c3e7-112e-4c7c-fa77-d3c629ebe97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Loader**"
      ],
      "metadata": {
        "id": "Vb8QxpTkBzXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class ModelLoader:\n",
        "    def __init__(self):\n",
        "        # Load models and tokenizers\n",
        "        self.models = {\n",
        "            \"BSJCode-1-Stable\": self.load_model(\"BSAtlas/BSJCode-1-Stable\"),\n",
        "            \"CodeLlama\": self.load_model(\"codellama/CodeLlama-7b-Instruct-hf\"),\n",
        "            \"Terjman\": self.load_model(\"atlasia/Terjman-Ultra\")\n",
        "\n",
        "        }\n",
        "        self.tokenizers = {\n",
        "            \"BSJCode-1-Stable\": AutoTokenizer.from_pretrained(\"BSAtlas/BSJCode-1-Stable\"),\n",
        "            \"CodeLlama\": AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
        "            \"Terjman\": AutoTokenizer.from_pretrained(\"atlasia/Terjman-Ultra\")\n",
        "        }\n",
        "\n",
        "    def load_model(self, model_name):\n",
        "        huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "        if not huggingface_token:\n",
        "            raise ValueError(\"Hugging Face token is not set. Please configure it.\")\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        return AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=huggingface_token)\n",
        "\n",
        "    def generate_response(self, model_name, input_text):\n",
        "        if model_name not in self.models:\n",
        "            raise ValueError(f\"Model {model_name} not loaded\")\n",
        "\n",
        "        tokenizer = self.tokenizers[model_name]\n",
        "        model = self.models[model_name]\n",
        "\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HodEvTKRA-T2",
        "outputId": "f71b3e73-d72f-4cb8-8722-ba87e9cd4a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flask API**"
      ],
      "metadata": {
        "id": "66RF9VacCZaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, request, jsonify\n",
        "from models import ModelLoader\n",
        "\n",
        "app = Flask(__name__)\n",
        "model_loader = ModelLoader()\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        data = request.json\n",
        "        user_input = data.get(\"input\", \"\")\n",
        "        service_type = data.get(\"service\", \"BS-friendly\")\n",
        "\n",
        "        if not user_input:\n",
        "            return jsonify({\"error\": \"Input text is required\"}), 400\n",
        "\n",
        "        if service_type == \"BS-friendly\":\n",
        "            response = model_loader.generate_response(\"BSJCode-1-Stable\", user_input)\n",
        "        elif service_type == \"Pro\":\n",
        "            response = model_loader.generate_response(\"CodeLlama\", user_input)\n",
        "        elif service_type == \"Premium\":\n",
        "            # Process input through BS model\n",
        "            intermediate_bs = model_loader.generate_response(\"BSJCode-1-Stable\", user_input)\n",
        "            # Refine BS output using CodeLlama\n",
        "            intermediate_cl = model_loader.generate_response(\"CodeLlama\", intermediate_bs)\n",
        "            # Clarify output in Darija using Terjman\n",
        "            response = model_loader.generate_response(\"Terjman\", intermediate_cl)\n",
        "        else:\n",
        "            return jsonify({\"error\": \"Invalid service type\"}), 400\n",
        "\n",
        "        return jsonify({\"output\": response}), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000)"
      ],
      "metadata": {
        "id": "iaM3ixFBpnve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed5f346-38fe-4679-8e62-fc37f26cd4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the app**"
      ],
      "metadata": {
        "id": "pURvE8CTDdfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d4ZPXJqWDiG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exposing the end-point**"
      ],
      "metadata": {
        "id": "nHOoU1QuCd_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "# In the left side bar, you can find a key logo, click on it and create your Hugging Face access token key variable\n",
        "NG_TOKEN = userdata.get('NG_TOKEN')\n",
        "os.environ['NG_TOKEN'] = NG_TOKEN\n",
        "print(NG_TOKEN)"
      ],
      "metadata": {
        "id": "KTrlR3AnL9BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start the Flask app in the background using subprocess\n",
        "process = subprocess.Popen(['python', 'app.py'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "ngrok.set_auth_token(NG_TOKEN)\n",
        "\n",
        "# Wait for ngrok to establish a tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Flask app is running at: {public_url}\")"
      ],
      "metadata": {
        "id": "cQXv1-c0Y9Gi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}